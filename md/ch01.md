# Chapter 1: Mathematics of Prediction

Home: [Course Index](index.html)

_[Aman Bhargava](aman-bhargava.com), Fall 2024_

Probability distributions quantify the likelihood of events. 
Modern language models are chiefly concerned with approximating the probability distribution of sequences of well-formed text. 
Once a high-quality approximation of the probability distribution of text sequences is learned, text can be generated by sampling the distribution. 
A basic understanding of probability distributions ([Section 1.1](#probability-distributions)) is key to working with large language models. 
We will then provide an overview of basic methods (ML, MAP) for estimating probability distributions in [Section 1.2](#maximum-likelihood-estimation).





## 1.1: Probability Spaces {#probability-spaces}

**Definition**: Probability space -- a probability space is a mathematical framework to model random experiments that consists of three elements: 

 1. **Sample space $\Omega$**: The set of all possible outcomes of a random experiment. 
 2. **Sigma-algebra ($\mathcal F$)**: A collection of subsets of $\Omega$ including $\Omega$ itself and the empty set $\varnothing$ that satisfies the axioms of $\sigma$-algebras (e.g., closure under complement and countable unions), representing the events or "outcomes" we can assign probabilites to. 
 3. **Probability measure ($P$)**: A function $P: \mathcal F \to [0, 1]$ that assigns probabilities to events in $\mathcal F$. 

The probability measure $P$ must satisfy the **Kolmogorov axioms**: 

 1. **Non-negativity**: For any event $A \in \mathcal F$, $$P(A) \geq 0$$
 2. **Normalization**: $$P(\Omega) = 1$$. 
 3. **Countable Additivity**: For any countable sequence of pairwise disjoint events $\{A_i\}_{i=1}^\infty \subset \mathcal F$ (i.e. $A_i \cap A_j = \varnothing$ for $i\neq j$).

These axioms set the stage for deriving important properties of probability distributions. E.g., for any two events $A, B \in \mathcal F$, $$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$


### Random Variables

**Definition**: Random variable -- a random variable is a measurable function that maps outcomes from a random experiment to real numbers: $$X: \Omega \to \mathbb R$$ 
Random variables quantify random phenomena and are central to probability and statistics. 

 - **Discrete random variables**: Take on countable values (e.g., integers). 
 - **Continuous random variables**: Take on values from an interval or collection of intervals in $\mathbb R$. 

**Notation**: Statistics and probability theory might has irritating notation, but we are stuck with it. 

 - **Uppercase letters** (e.g., $X$) denote random variables. 
 - **Lowercase letters** (e.g., $x$) denote specific values or "realizations" of the random variable $X$. We might say $x$ is sampled from $X$ or a function of $X$ as $x\sim X$ or $x \sim g(X)$. You may also see $x\sim f_X(x)$ or $x\sim p_X(x)$ when attention is being drawn to the probability distribution of random variable $X$. 

**Definition**: Support of random variable $S_X$ -- the support or range of a random variable $X$ is 
$$S_X = \{x \in \mathbb R \mid \exists \omega \in \Omega \text{ such that } X(\omega) = x\}$$


## 1.2 Probability Distributions

A probability distribution describes how probabilities are assigned over the potential values of a random variable. 

### Discrete Random Variables and Probability Mass Functions (PMFs)

For a discrete random variable $X$ with support $S_X$, the **probability mass function** (PMF) $p_X(x)$ gives the probability $\Pr\{X = x\}$. PMFs obey the following properties: 

 1. **Non-negativity**: $p_X(x) \geq 0$  $\forall$ $x \in S_X$. 
 2. **Normalization**: $$\sum_{x\in S_X} p_X(x) = 1$$
 3. **Additivity**: For any countable collection of disjoint values $\{x_i\}$, $$\Pr \big( X \in \{x_i\} \big) = \sum_{x_i} p_X(x_i)$$


### Continuous Random Variables and Probability Density Functions

For a continuous random variable $X$, the **probability density function** (PDF) $f_X(x)$ is a function such that the probability such that $X$ falls within interval $[a, b]$ is given by 
$$\Pr\big(a \leq X \leq b\big) = \int_a^b f_X(x) dx$$

**Properties of PDFs**: 

 1. **Non-negativity**: $f_X(x) \geq 0$ $\forall x \in \mathbb R$
 2. **Normalization**: $$\int_{-\infty}^{\infty} f_X(x) dx = 1$$
 3. **Probability over intervals**: Probabilities are calculated over intervals, not at individual points as applying $\Pr\big(a \leq X \leq b\big) = \int_a^b f_X(x) dx$ to an interval of zero length gives zero probability. 

The name for the value of the PDF $f_X(x)$ at a particular value of $x$ is called **"likelihood"** rather than **"probability"**. 
If you confuse these, it will anger the statisticians. 


### Laws of Large Numbers


Recall the definitions of expected value for continuous random variable $X$ is $\mu = \mathbb E[X] = \int_{-\infty}^{\infty} x \cdot f_X(x)$. For $X$, we have $\mu = \mathbb E[X] = \sum_{x\in S_X} x\cdot p_X(x)$. 


**Weak law of large numbers** (Khinchin's law): Let $\{X_i\}_{i=1}^n$ be a sequence of _independent and identically distributed_ (i.i.d.) random variables with finite expected value $\mu = \mathbb E[X_i]$. Then, for any $\epsilon > 0$, 
$$
\lim_{n\to \infty} \Pr\begin{pmatrix}
\big|
\frac 1 n \sum_{i=1}^n X_i - \mu
\big|
\geq \epsilon
\end{pmatrix} = 0
$$

**Strong law of large numbers**: If $\{x_i\}_{i=1}^n$ are sampled from i.i.d. random variables with mean $\mu = \mathbb E[X_i]$, then the probability of the sample mean = $\mu$ is $1$ as $n \to \infty$. 
$$
\Pr\begin{pmatrix}
\lim_{n\to \infty} \frac 1 n \sum_{i=1}^n X_i = \mu
\end{pmatrix} = 1
$$


### Conditional Distributions

Joint distributions for two random variables are denoted as $f_{XY}(x, y)$ for continuous random variables and $p_{XY}(x, y)$. 
For brevity, we will refer to each without the subscripts as $f(x, y)$ and $p(x, y)$. 
Conditional distributions are denoted $f(y | x)$ or $p(y | x)$ and are interpreted as the probability distribution over $y$ given a known value of $x$. 

$$
f(y | x) = f(x, y) / f(x)
$$

$$
p(y | x) = p(x, y) / p(x)
$$

**Independence**: Random variables $X, Y$ are independent iff $p(x, y) = p(x) p(y)$. This also implies that $p(y | x) = p(y)$. 

**Conditional Independence**: 



### Chain Rule of Probability

Let us break free from the assumption of independence in a sequence of random variables $\{X_i\}_{i=1}^n$. 
If we know that the process underlying $\{X_i\}$ is **causal** -- i.e., $X_i$ is conditional only on $X_1, \dots, X_{i-1}$, then we can use this handy trick to factor the joint probability of the sequence $p(x_1, \dots, x_n)$. 
as 

$$
p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i | x_{< i})
$$

where $x_{< i}$ denotes $x_1, \dots, x_{i-1}$. The chain rule of probability is especially handy for modelling language as a random process. 


**Derivation**: By repeatedly applying the chain rule of probability and invoking our causal assumption ($X_i$ is only conditional on $X_{< i}$), we get 

$$
p(x_1) = p(x_1)
$$

$$
p(x_1, x_2) = p(x_2 | x_1) p(x_1)
$$

$$
p(x_1, x_2, x_3) = p(x_3 | x_1, x_2) p(x_1, x_2)
$$

$$
 = p(x_3 | x_1, x_2) \underbrace{p(x_2 | x_1) p(x_1)}_{\equiv p(x_1, x_2)}
$$

And so on, until we derive the general form. 

**Note**: While I have used discrete/PMF notation $p(x_1, \dots, x_n)$, the chain rule also applies to causal random processes with continuous variables $f(x_1, \dots, x_n)$. 





## 1.2: Estimating Probability Distributions

Abstract probability distributions like $p(x)$, $p(y | x)$, and $p(x_i | x_1,\dots, x_{i-1})$ are great to think about, but we often are not given the exact form of $p(x)$ initially. 
More commonly, we have a bunch of samples from the distribution in question, and we need to estimate $p(x)$ from samples $x_i \sim p(x)$ or $x_i \sim f(x)$. 

**Parameterized probability distributions**: We denote the parameters of a probability distribution as $\theta$, and the parameterized distribution as $p(x; \theta)$ or $f(x; \theta)$. You may also see the equivalent notation $p_{\theta}(x)$ or $f_{\theta}(x)$ commonly used. 


**Goal of parameter estimation**: Once we have figured out which family of distributions we are interested in, we want to estimate the best parameters $\theta$ that explain our measurements. 

**Simplest parameter estimation example**: Let's say we have a bunch of i.i.d. samples $x_1, \dots, x_n$ sampled from some unknown Bernoulli random variable $X$. 
Let $r$ denote the "ground truth" 



### Maximum Likelihood Estimation

### Maximum a Posteriori Estimation

### Log Likelihood Loss Function




## 1.3: Shannon N-Gram Models



