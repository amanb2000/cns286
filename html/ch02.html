<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ch02</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/home/odysseus/cns286/style.css" />
</head>
<body>
<h1 id="chapter-2-learning-predictive-models">Chapter 2: Learning Predictive Models</h1>
<p>Home: <a href="index.html">Course Index</a></p>
<p><em><a href="aman-bhargava.com">Aman Bhargava</a>, Fall 2024</em></p>
<p>Using machines to learn predictive models turns out to be a bright idea. By using lots of data to infer parameters for the right class of model, you can solve a lot of problems.</p>
<ul>
<li>Supervised vs.Â unsupervised learning</li>
</ul>
<h2 id="linear-and-logistic-regression">2.1: Linear and Logistic Regression</h2>
<h2 id="likelihood-maximization-as-an-optimization">2.2: Likelihood Maximization as an Optimization</h2>
<h2 id="loss-functions-and-gradient-descent">2.3: Loss Functions and Gradient Descent</h2>
<h3 id="learning-as-function-approximation">Learning as Function Approximation</h3>
<h3 id="venturing-beyond-linearity">Venturing Beyond Linearity</h3>
<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>
<h2 id="neural-networks">2.4: Neural Networks</h2>
<h3 id="chain-rule-of-calculus">Chain Rule of Calculus</h3>
<h3 id="miracle-of-deep-learning">Miracle of Deep Learning</h3>
<h3 id="the-bitter-lesson">The Bitter Lesson</h3>
</body>
</html>
