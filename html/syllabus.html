<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>syllabus</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/home/odysseus/cns286/style.css" />
  <script src="/usr/share/javascript/mathjax/tex-mml-chtml.js" type="text/javascript"></script>
</head>
<body>
<h1 id="cns286-syllabus-fall-2024">CNS286 Syllabus Fall 2024</h1>
<p><a href="https://lancelot.languagegame.io/cns286/index.html">Course Index</a></p>
<p><em>Aman Bhargava, Thomson Lab, Caltech</em></p>
<ul>
<li><strong>Meetings</strong>: Thursdays 5:30-7pm.</li>
<li><strong>Units</strong>: 6 hours per week.</li>
<li><strong>Grading</strong>: 3 assignments with code review.</li>
</ul>
<h2 id="lectures-slides">Lectures + Slides</h2>
<ol type="1">
<li><strong>Mathematics of Prediction</strong>:
<ul>
<li>Review: Probability distributions, conditional distributions and likelihood.</li>
<li>Shannon N-gram models.</li>
<li>Parameter estimation (ML, MAP), statistical inference.</li>
<li>Chain rule of probability for sequential prediction, log-likelihood loss function.</li>
<li><em>Bonus: Markov models, hidden Markov models</em></li>
</ul></li>
<li><strong>Computational Predictive Models and Learning</strong>:
<ul>
<li>Parameter estimation <span class="math inline">\(\sim\)</span> optimization <span class="math inline">\(\sim\)</span> learning <span class="math inline">\(\sim\)</span> function approximation.</li>
<li>Linear regression as a model learning system.</li>
<li>Loss functions, gradient descent.</li>
<li>Neural networks and the chain rule of calculus (backpropagation).</li>
<li>The miracle of deep learning and the bitter lesson.</li>
</ul></li>
<li><strong>Sequence Prediction with Recurrent Neural Networks</strong>:
<ul>
<li>Markov processes as canonical sequential statistical processes.</li>
<li>Challenge of sequential prediction with neural networks.</li>
<li>Latent states for sequence prediction.</li>
<li>Backpropagation through time.</li>
<li>Problems with backpropagation through time.</li>
</ul></li>
<li><strong>Attention is all you need</strong>:
<ul>
<li>Self-attention for sequence modelling.</li>
<li>Transformer blocks as multi-head self-attention + nonlinearities.</li>
<li>Masked language modelling with transformers (BERT).</li>
<li>Causal auto-regressive language modelling with transformers (GPT).</li>
</ul></li>
<li></li>
</ol>
</body>
</html>
