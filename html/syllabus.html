<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>syllabus</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/home/odysseus/cns286/style.css" />
</head>
<body>
<h1 id="cns286-syllabus-fall-2024">CNS286 Syllabus Fall 2024</h1>
<p>Home: <a href="https://lancelot.languagegame.io/cns286/index.html">Course Index</a></p>
<p><em>Aman Bhargava, Thomson Lab, Caltech</em></p>
<ul>
<li><strong>Meetings</strong>: Thursdays 5:30-7pm.</li>
<li><strong>Units</strong>: 6 hours per week.</li>
<li><strong>Grading</strong>: Projects with code review and discussion.</li>
</ul>
<h2 id="lectures-slides">Lectures + Slides</h2>
<p>The topics in this course can be roughly broken down into two halves: conceptual and practical.</p>
<h3 id="conceptual-topics">Conceptual Topics</h3>
<ol type="1">
<li><strong>Mathematics of Prediction</strong>: <a href="ch01.html">Chapter 1 Slides</a>
<ul>
<li>Review: Probability distributions, conditional distributions and likelihood.</li>
<li>Statistical inference.</li>
<li>Shannon N-gram models.</li>
<li>Parameter estimation (ML, MAP).</li>
<li>Chain rule of probability for sequential prediction.</li>
<li>Log-likelihood loss function for sequences.</li>
<li><em>Bonus: Markov models, hidden Markov models</em></li>
<li><strong>Project 1</strong>: Build a dictionary-based N-gram++ character language model.</li>
</ul></li>
<li><strong>Computational Predictive Models and Learning</strong>:
<ul>
<li>Parameter estimation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math> optimization <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math> learning <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math> function approximation.</li>
<li>Linear regression as a model learning system.</li>
<li>Loss functions, gradient descent.</li>
<li>Neural networks and the chain rule of calculus (backpropagation).</li>
<li>The miracle of deep learning and the bitter lesson.</li>
<li><strong>Project 2</strong>: Linear and MLP N-gram models in PyTorch.</li>
</ul></li>
<li><strong>Sequence Prediction with Recurrent Neural Networks</strong>:
<ul>
<li>Markov processes as canonical sequential statistical processes.</li>
<li>Challenge of sequential prediction with neural networks.</li>
<li>Latent states for sequence prediction.</li>
<li>Backpropagation through time.</li>
<li>Problems with backpropagation through time.</li>
<li><strong>Project 3</strong>: Character-based RNN in PyTorch.</li>
</ul></li>
<li><strong>Attention is all you need</strong>:
<ul>
<li>Self-attention for sequence modelling.</li>
<li>Transformer blocks as multi-head self-attention + nonlinearities.</li>
<li>Masked language modelling with transformers (BERT).</li>
<li>Causal auto-regressive language modelling with transformers (GPT).</li>
</ul></li>
</ol>
<h3 id="practical-topics">Practical Topics</h3>
<ol start="5" type="1">
<li><strong>Build a Generative Pre-Trained Transformer from Scratch</strong>:
<ul>
<li><strong>Project 4</strong>: Follow along with Andrej Karpathy’s <a href="https://youtu.be/kCc8FmEb1nY?si=k4UW-QmI4bM5r3ky">GPT tutorial</a>.</li>
</ul></li>
<li><strong>Train GPT-2 HuggingFace Template Model</strong>:
<ul>
<li><strong>Project 5</strong>: Recapitulate your results from the transformer you build from scratch with your own <a href="https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/models/gpt2/modeling_gpt2.py#L1179">newly-trained GPT2LMHeadModel</a>.</li>
</ul></li>
<li><strong>Use and Fine-Tune Pre-Trained Transformers</strong>:
<ul>
<li><strong>Project 6</strong>: Prompt HuggingFace pre-trained autoregressive transformer LLMs for various tasks, apply low-rank fine-tuning (LoRA) to efficiently modify behavior.</li>
</ul></li>
<li><strong>Learn to use LLM APIs Properly</strong>: HTTP requests, asynchronous scripting for high-throughput tasks, best practices for organizing scripts.
<ul>
<li><strong>Project 7</strong>: Build an auto-grader for short-answer text responses to high school science questions.</li>
</ul></li>
<li><strong>Hack and Control Transformers</strong>: Explore token-based and activation based control methods.
<ul>
<li>Human and LLM-based prompt engineering.</li>
<li>1st-order discrete prompt optimization (<a href="https://github.com/amanb2000/Magic_Words">greedy coordinate gradient</a>).</li>
<li>Soft prompting (1st-order continuous prompt optimization).</li>
<li>Try perturbing activations by modifying the <a href="https://github.com/huggingface/transformers/blob/69b5ccb8878b58372ea326d17d9490d67ccf23a7/src/transformers/models/gpt2/modeling_gpt2.py#L725">past key and value representations</a>.</li>
</ul></li>
<li><strong>Analyze LLM Representations</strong>: Visualize activations, apply linear probes to analyze informational content of representations, analyze geometry and structure in representations (<a href="https://colab.research.google.com/drive/1NQseiwja2wTP4dpN4i8g1LxRo4_af3i4?usp=sharing">starter code</a>).
<ul>
<li><a href="https://colab.research.google.com/drive/1DeagoR31QM9qFsMkVMgEuJPst9OwsGv4">Sparse autoencoders for interpretability</a> with Anthropic.</li>
</ul></li>
</ol>
</body>
</html>
