<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ch01</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<h1 id="chapter-1-mathematics-of-prediction">Chapter 1: Mathematics of Prediction</h1>
<p>Home: <a href="index.html">Course Index</a></p>
<p><em><a href="aman-bhargava.com">Aman Bhargava</a>, Fall 2024</em></p>
<p>Probability distributions quantify the likelihood of events. Modern language models are chiefly concerned with approximating the probability distribution of sequences of well-formed text. Once a high-quality approximation of the probability distribution of text sequences is learned, text can be generated by sampling the distribution. A basic understanding of probability distributions (<a href="#probability-distributions">Section 1.1</a>) is key to working with large language models. We will then provide an overview of basic methods (ML, MAP) for estimating probability distributions in <a href="#maximum-likelihood-estimation">Section 1.2</a>.</p>
<h2 id="probability-distributions">1.1: Probability Distributions</h2>
<ul>
<li>Axioms</li>
</ul>
<h3 id="conditional-distributions">Conditional Distributions</h3>
<ul>
<li>Definition</li>
</ul>
<h3 id="chain-rule-of-probability">Chain Rule of Probability</h3>
<ul>
<li>Definition</li>
</ul>
<h2 id="estimating-probability-distributions">1.2: Estimating Probability Distributions</h2>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>;</mo><mi>Î¸</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(x; \theta)</annotation></semantics></math></li>
</ul>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<h3 id="maximum-a-posteriori-estimation">Maximum a Posteriori Estimation</h3>
<h3 id="log-likelihood-loss-function">Log Likelihood Loss Function</h3>
<h2 id="shannon-n-gram-models">1.3: Shannon N-Gram Models</h2>
</body>
</html>
